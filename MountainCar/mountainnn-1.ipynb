{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b68480b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pip install torch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03aea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())#take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeight(X_position):\n",
    "    return np.sin(3 * X_position) * .45 + .55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newreward(pos):\n",
    "    if (pos >= 0.5):\n",
    "        return 2\n",
    "    else:\n",
    "        return (pos+1.2)/1.8 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043744db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, learning, epsilon, min_eps, episodes):\n",
    "    #Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 50])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    #Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 0,\n",
    "                          size = (num_states[0], num_staes[1],\n",
    "                                 env.action_space.n))\n",
    "    Qinit = np.copy(Q)\n",
    "    \n",
    "    #Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    #Make copy of epsilon\n",
    "    eps1 = epsilon\n",
    "    \n",
    "    #Keep track of first success\n",
    "    first = episodes + 1\n",
    "    \n",
    "    #Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        #Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0.0\n",
    "        state = env.reset()\n",
    "        \n",
    "        #Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 50])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        \n",
    "        while done !=True:\n",
    "            #Render environment for last few episodes\n",
    "            if i >= (episodes - 5) or i<5:\n",
    "                env.render()\n",
    "                \n",
    "        #Determine next action - epsilon greedy strategy\n",
    "        if np.random.random() < 1 - epsilon:\n",
    "            action = np.argmax(Q[state_adj[0], state_dj[1]])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        #Get next state and reward\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Discretize state2\n",
    "        state2_adj = (state2 - env.observation_space.low)*np.array([10, 50])\n",
    "        state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "        \n",
    "        #Save tp Qpoints\n",
    "        row = np.array([state2_adj, [0],state_adj[1],action])\n",
    "        \n",
    "        #Allow foe terminal states\n",
    "        if done and state2[0] >= 0.5:\n",
    "            Q[state_adj[0], state_adj[1], action] = reward\n",
    "            \n",
    "        #Adjust Q value for current state\n",
    "        else:\n",
    "            delta = learning*(newreward(state2[0]) + np.max(Q[state2_adj[0], state2_adj[1]]) - Q[state_adj[0],\n",
    "             Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                                                                                 \n",
    "        #Notifies of any clears\n",
    "        if state2[0]>=0.5 and i<first:\n",
    "            first = i\n",
    "            print('first clear on episode {}'.format(i+1))\n",
    "            \n",
    "        #Update variables\n",
    "        tot_reward += newreward(state2[0])\n",
    "        state_adj = state2_adj\n",
    "                                                                                                 \n",
    "        #Update variables\n",
    "        tot_reward += newreward(state2[0])\n",
    "        state_adj = state2_adj\n",
    "                                                                                                 \n",
    "        #Decay epsilon\n",
    "        if epsilon > min_eps\n",
    "            epsilon *=eps1\n",
    "                                                                                                 \n",
    "        #Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "                                                                                                 \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "                                                                                                 \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('episode {} Average Reward: {}'. format(i+1, ave_reward))\n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list, Q, Qinit\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208181ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2295db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b802996",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['line', 'blue', 'red']\n",
    "lables = ['left', 'nothing', 'right']\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "colorcount2 = np.array([0,0,0])\n",
    "colorcount = np.array([0,0,0])\n",
    "for i in range(19):\n",
    "    for j in range(8):\n",
    "        maxdex2 = np.argmax(Qinit[i][j])\n",
    "        ax1.ploy(i/10-1.2,j/50-0.07,'o',colors[maxdex2])\n",
    "        colorcount2[maxdex2] +=1\n",
    "        maxdex = np.argmax(Qpts[i][j])\n",
    "        ax2.plot(i/10-1.2,j/50-0.07,'o',color=colors[maxdex])\n",
    "        colorcount[maxdex]=1\n",
    "print('Initial action counts: {}' .format(colorcount2))\n",
    "print('Finial action counts: {}' .format(colorcount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c8086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc75510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
